{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Pipeline used in this project. \n",
    "\n",
    "- Using Random Forest Classifier\n",
    "- Nested Cross-Validation with Leave-One-Out (leave one participant out for testing)\n",
    "- Testing algorithm at 3 moments during the video (beggining, middle, end)\n",
    "- Predicting the type of the video (did the participant rate the video as: neutral, funny or very funny)\n",
    "- Using permutation test to define the luck threshold\n",
    "- 7 features:\n",
    "    - Participant Smiles: AU6, AU12, AU25\n",
    "    - Videos: Semantic, NGD, Saliecy, Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit, LeaveOneGroupOut, RandomizedSearchCV, permutation_test_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use for (participant x video) for X time-cluster + use funny\n",
    "\"\"\"\n",
    "\n",
    "def create_data(temps, csv):\n",
    "    df = pd.read_csv(csv).drop(columns=['Unnamed: 0', 'video']).dropna()\n",
    "\n",
    "    df = df.loc[df['video_section']==temps[0]]\n",
    "    df = df.loc[:, ~df.columns.str.startswith(temps[1])]\n",
    "    df = df.loc[:, ~df.columns.str.startswith(temps[2])]\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    X_df = df.drop(columns=['funny_quantile','video_section', 'siteweb_type'])\n",
    "    X = X_df.values\n",
    "\n",
    "    y = df['funny_quantile'].values\n",
    "\n",
    "    groups = df['participantID'].values\n",
    "    index_participant = list(X_df).index('participantID')\n",
    "\n",
    "    X_df = X_df.rename(columns={'T{}X0Y0_Movement'.format(temps[3]):'Movement', 'T{}X0Y0_Saliency'.format(temps[3]):'Saliency','Anomaly':'Anomalie','Mean_DS_Google':'Semantic Distance'})\n",
    "    print(list(X_df))\n",
    "    X_df\n",
    "\n",
    "    feature_names = list(X_df)\n",
    "    feature_names.remove('participantID')\n",
    "\n",
    "    \n",
    "    return X, y, groups, index_participant, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create empty Dataframe to store results\n",
    "df_result = pd.DataFrame()\n",
    "temps_all = [[1, 'T1', 'T2', '0', 'Debut'], [2, 'T0', 'T2', '1', 'Milieu'], [3, 'T0', 'T1', '2', 'Fin']]\n",
    "\n",
    "csv = './dataset_prediction_funny.csv'\n",
    "cv= 10\n",
    "bootstrap= 100\n",
    "n_iter = 100\n",
    "\n",
    "# Iterate trough time\n",
    "# for time in range(len(temps_all)):\n",
    "for time in temps_all:\n",
    "    print('\\n\\n**** Computation of Time Cluster :', time[4], '****\\n\\n')    \n",
    "    X, y, groups, index_participant, feature_names = create_data(time, csv)\n",
    "    \n",
    "    \"\"\"Step 1 : split into 10-90 test-train groups for n_split time\"\"\"\n",
    "    gss = GroupShuffleSplit(n_splits=cv, test_size=0.1, random_state=0)\n",
    "    i=0\n",
    "    for train_index, test_index in gss.split(X, y, groups):\n",
    "        print('Cross-Validation:',i+1,'on',cv)\n",
    "        # Create the Xth train-test set\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Keep the participant IDs for the Leave-P-Group after\n",
    "        train_groups = X_train[:,index_participant]\n",
    "        test_groups  = X_test[:,index_participant]\n",
    "\n",
    "        # Remove participant IDs from the data\n",
    "        X_train = np.delete(X_train, index_participant,1)\n",
    "        X_test  = np.delete(X_test, index_participant,1)\n",
    "        \n",
    "        print('Participant_Id for validation:', list(set(groups[test_index])))\n",
    "\n",
    "\n",
    "        \"\"\"Step 2: test hyperparameters with CV=leave-One-Group-Out\"\"\"\n",
    "\n",
    "        logo = LeaveOneGroupOut()\n",
    "        rfc = RandomForestClassifier()\n",
    "        param_RFC = {\n",
    "            'n_estimators': [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)],\n",
    "            \"criterion\": ['gini', 'entropy'], \n",
    "            'max_depth': range(1,100), \n",
    "            'min_samples_split': range(1,100),\n",
    "            'max_leaf_nodes': range(1,100),\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_leaf': [int(x) for x in np.linspace(start = 1, stop = 200, num = 5)],\n",
    "            'bootstrap': [True, False]}\n",
    "\n",
    "        clf = RandomizedSearchCV(\n",
    "            estimator = rfc,\n",
    "            param_distributions = param_RFC, n_iter = n_iter,\n",
    "            scoring='accuracy', n_jobs=-1, cv=logo, refit=True, return_train_score = True, random_state=0, verbose=3)\n",
    "\n",
    "        best_clf = clf.fit(X_train, y_train, train_groups);\n",
    "\n",
    "\n",
    "        \"\"\" Step 3: train the best algorithm on all data\"\"\"\n",
    "        print('Best algorithm found. Training algorithm on new data')\n",
    "        # Train rfc with the best hyperparameters\n",
    "        best_rfc = RandomForestClassifier(**clf.best_params_)\n",
    "        best_rfc.fit(X_train, y_train) \n",
    "        \n",
    "        disp = plot_confusion_matrix(best_rfc, X_test, y_test)\n",
    "        disp2 = plot_confusion_matrix(best_rfc, X_test, y_test, normalize='true', cmap=plt.cm.Blues)\n",
    "        plt.show()\n",
    "\n",
    "        print(classification_report(y_test, best_rfc.predict(X_test)))\n",
    "        \n",
    "        print('Accuracy for training set:', round(best_rfc.score(X_train, y_train), 3))\n",
    "        print('Accuracy for validation set:', round(best_rfc.score(X_test, y_test), 3))\n",
    "              \n",
    "        # Get best features\n",
    "        features_score = {feature_names[i]: best_rfc.feature_importances_[i] for i in range(len(feature_names))}\n",
    "\n",
    "        # Prepare new row for Dataframe\n",
    "        new_row=clf.best_params_\n",
    "        new_row['time']=time[0]\n",
    "        new_row['iteration']=i\n",
    "        new_row['accuracy']= round(best_rfc.score(X_test, y_test), 3)   \n",
    "        \n",
    "        \"\"\" Step 4: Create bootstraping\"\"\"\n",
    "        print('Trying {} permutations'.format(bootstrap))\n",
    "        score, permutation_scores, pvalue = permutation_test_score(best_rfc, X_train, y_train, groups=train_groups, scoring=\"accuracy\", cv=5, n_permutations=bootstrap, n_jobs=-1 ,verbose=3)\n",
    "\n",
    "        new_row['permutation_score'] = score\n",
    "        new_row['permutation_pvalue'] = pvalue\n",
    "        new_row['permutation_mean'] = permutation_scores.mean()\n",
    "        new_row['permutation_std'] = permutation_scores.std()\n",
    "        \n",
    "        #Merge row and append to df\n",
    "        new_row = {**new_row, **features_score}\n",
    "        df_result = df_result.append(new_row, ignore_index=True)\n",
    "        i+=1\n",
    "        \n",
    "        # Plot Permutation tests\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(permutation_scores, bins=20, density=True, color='grey')\n",
    "        ax.axvline(score, ls='--', color='r')\n",
    "        score_label = (f\"Score on original\\ndata: {score:.2f}\\n\"\n",
    "                       f\"(p-value: {pvalue:.3f})\")\n",
    "        plt.show()\n",
    "\n",
    "#         df_result.to_csv('./results_LOGO/prediction_funny.csv')\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_result[['AU6','AU12', 'AU25','Semantic Distance','NGD','Movement','Saliency']].T.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# df_result = pd.read_csv('./results_LOGO/prediction_funny.csv')\n",
    "# df_result = df_result.replace({\"time\":{1:'Beggining', 2:'Middle', 3:'End'}})\n",
    "# feature_names  = ['AU12', 'AU25', 'AU6', 'Movement', 'NGD', 'Saliency', 'Semantic Distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(df_train.drop(columns=['funny_quantile','video']))\n",
    "f = feature_names+['accuracy', 'time', 'iteration']\n",
    "df_plot =df_result[f].melt(id_vars=['time', 'iteration', 'accuracy'])\n",
    "\n",
    "df_plot['type'] = 'video'\n",
    "df_plot.loc[df_plot['variable'] == 'AU6' , 'type'] = 'smile' # Funny\n",
    "df_plot.loc[df_plot['variable'] == 'AU12' , 'type'] = 'smile' # Funny\n",
    "df_plot.loc[df_plot['variable'] == 'AU25' , 'type'] = 'smile' # Funny\n",
    "\n",
    "sns.relplot(x=\"time\", y=\"value\", kind=\"line\", data=df_plot, hue='variable', ci='sd', color='dark2', style='type')\n",
    "\n",
    "\n",
    "from anova import*\n",
    "perm_value = 0.33#df_result['permutation_max'].max()\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "ax.axhline(perm_value, ls='--', color='red')\n",
    "\n",
    "Anova_Improved(df_result, 'accuracy', group='time', alpha=0.05, ylim=0, graph=True, order=None, color='gray')\n",
    "    \n",
    "print(df_plot.groupby('time').mean())\n",
    "print(df_plot.groupby('time').std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.groupby('time').max().round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
